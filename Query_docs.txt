SYSTEM ROBUSTNESS TEST QUERIES - THREE DISTINCT INPUT SCENARIOS

================================================================================
(I) COMPLETE QUERIES WITH FULLY SPECIFIED INPUTS
================================================================================

1. Basic Word Count Pipeline (Complete Version):
Create an Apache Flink streaming application that processes text data with the following specifications:
- Source: Kafka topic "input-text" (bootstrap servers: localhost:9092, consumer group: word-count-group)
- Input format: Plain text messages with UTF-8 encoding
- Processing: Split messages by whitespace regex "\\s+", convert to lowercase, filter words with length >= 3
- Windowing: 30-second tumbling windows for aggregation
- Output: Local file system at "/output/word-counts.txt" with format "word,count,timestamp"
- Parallelism: 4 for source, 8 for processing, 2 for sink
- Checkpointing: Every 10 seconds with RocksDB state backend
- Error handling: Dead letter queue for malformed messages to Kafka topic "dlq-text"

2. CSV Data Transformation Pipeline (Complete Version):
Implement an Apache Flink ETL pipeline with these exact requirements:
- Source: File system directory "/input/csv/" monitoring for new CSV files every 5 seconds
- Input schema: id(Long), name(String), email(String), age(Integer), salary(Double), department(String)
- Validation rules: age between 18-65, email must contain "@" and ".", salary > 0
- Transformations: Convert to JSON with renamed fields {id→employee_id, name→full_name, email→contact_email}, add current timestamp as "processed_at"
- Output: Kafka topic "employee-json" (serialization: JSON, partitioning by department)
- Error handling: Invalid records to "/error/invalid-records.log" with rejection reasons
- Parallelism: Source=1, Transform=6, Sink=3
- Exactly-once semantics with 5-second checkpoint intervals

3. Simple Log Aggregator (Complete Version):
Build a Flink application for log analysis with these specifications:
- Source: Kafka topic "application-logs" (bootstrap servers: kafka1:9092,kafka2:9092)
- Input format: JSON with fields {timestamp(ISO-8601), level(String), message(String), service(String)}
- Window configuration: 5-minute tumbling windows based on event time
- Watermark strategy: Bounded out-of-orderness with 30-second max delay
- Aggregation: Count by log level (DEBUG, INFO, WARN, ERROR, FATAL) per window
- Output schema: {window_start, window_end, service, level, count}
- Sink: PostgreSQL database (host: db-server, port: 5432, database: logs, table: log_aggregates)
- Late data handling: Side output to Kafka topic "late-logs"
- Parallelism: 4 throughout pipeline

4. Event Filtering Pipeline (Complete Version):
Develop a Flink event filtering application with these precise requirements:
- Source: Kafka topic "raw-events" (consumer group: filter-group, offset: earliest)
- Input schema: {eventId(String), eventType(String), userId(Long), payload(Map<String,Object>), timestamp(Long)}
- Filter conditions: eventType IN ["USER_LOGIN", "PURCHASE", "PAGE_VIEW"], userId NOT NULL, timestamp within last 24 hours
- Processing: Apply filters in sequence, log filtered events with reasons
- Output: Filtered events to Kafka topic "filtered-events" with added field "filter_passed_at"
- Rejected events: Kafka topic "rejected-events" with rejection reasons
- Metrics: Count of processed, accepted, rejected events per minute to InfluxDB
- Configuration: Externalized in application.properties file
- Parallelism: Source=2, Filter=8, Sink=4

5. Temperature Monitoring (Complete Version):
Create a temperature monitoring Flink application with these specifications:
- Source: Kafka topic "sensor-readings" (JSON format, event time from "reading_time" field)
- Input schema: {sensor_id(String), temperature(Double), humidity(Double), location(String), reading_time(ISO-8601)}
- Alert conditions: temperature > 30.0°C OR temperature < -10.0°C OR humidity > 85%
- Alert enrichment: Add severity level (CRITICAL >35°C, HIGH 30-35°C, LOW <-10°C), facility mapping from sensor location
- Window processing: 10-second tumbling windows for duplicate detection within same sensor
- Output format: {alert_id(UUID), sensor_id, temperature, location, severity, alert_time, facility_name}
- Destinations: Critical alerts to Kafka "critical-alerts", others to "standard-alerts"
- Persistence: All alerts to Elasticsearch index "temperature-alerts-{yyyy-MM-dd}"
- Dead letter queue: Malformed sensor data to "sensor-dlq" topic

6. Industrial Equipment Predictive Maintenance Pipeline (Complete Version):
Implement a comprehensive predictive maintenance Flink pipeline:
- Source: Kafka topic "factory-sensors" with schema {machine_id(String), temperature(Double), vibration_x(Double), vibration_y(Double), vibration_z(Double), pressure(Double), rpm(Integer), timestamp(Long)}
- Preprocessing: Filter out readings where any sensor value is null, temperature not in range [-50, 200], vibration values not in [-100, 100]
- Normalization: Z-score normalization per machine using 1-hour sliding window statistics
- Trend Detection: Linear regression over 15-minute sliding windows, flag if temperature slope > 2.0°C/minute
- Vibration Analysis: FFT analysis on 5-minute windows, detect anomalies using isolation forest algorithm (contamination=0.1)
- Risk Calculation: Weighted score = 0.4*temperature_trend + 0.6*vibration_anomaly_score
- Alert Generation: Risk score > 0.7 triggers JSON alert {machine_id, risk_score, contributing_factors, timestamp, recommended_action}
- Output: Alerts to file "/maintenance/alerts/alerts_{yyyy-MM-dd}.txt", metrics to InfluxDB measurement "equipment_health"
- State Management: RocksDB backend with 2-hour TTL for machine statistics
- Parallelism: 12 throughout pipeline

7. Real-Time Chat Filter Pipeline (Complete Version):
Develop a comprehensive chat moderation Flink application:
- Source: Kafka topic "raw-chat-messages" with schema {messageId(String), userId(String), channelId(String), text(String), timestamp(Long)}
- Profanity Filter: Use predefined list ["badword1", "badword2", "inappropriate"] with exact and fuzzy matching (Levenshtein distance ≤ 2)
- URL Filter: Regex (?:https?://)?(?:www\.)?(\w+\.\w+)(?:/\S*)? to detect URLs, check against blacklist ["malicious-site.com", "spam-domain.net"]
- Spam Filter: Detect if >5 identical messages from same user in 1-minute window OR message contains >3 repeated characters
- Severity Classification: SEVERE (profanity + URL), MODERATE (profanity OR URL), MILD (spam only)
- Message Routing: SEVERE → "/moderation/severe-violations.txt", MODERATE → "/moderation/moderate-violations.txt" with censored text, MILD/CLEAN → "/chat/filtered-messages.txt"
- Censoring Logic: Replace detected words with asterisks matching word length
- Metrics Output: Real-time dashboard metrics to Kafka topic "chat-metrics" every 30 seconds
- State: User message history with 5-minute TTL, channel-specific spam thresholds
- Parallelism: Source=4, Filter=12, Sink=6

8. Real-Time Image Compression Pipeline (Complete Version):
Create a sophisticated image processing Flink pipeline:
- Source: Kafka topic "raw-images" with schema {imageId(String), imageData(Base64 String), format(String), userId(String), uploadTime(Long)}
- Image Decoding: Base64 to byte arrays, validate formats (JPEG, PNG, GIF), extract metadata using ImageIO
- Metadata Extraction: {width, height, colorDepth, fileSize, compressionType, hasAlpha}
- Windowing: 5-second tumbling windows for batch processing, group by image format
- Lossless Compression: PNG optimization using deflate algorithm with compression level 9
- Lossy Compression: JPEG with quality factor 0.75, DCT-based compression with 8x8 block processing
- Quality Metrics: Calculate PSNR and SSIM scores comparing original vs compressed
- Routing Logic: Images >1MB → lossy compression, ≤1MB → lossless compression
- Watermarking: Embed invisible watermark using LSB steganography with timestamp and user ID
- Output Destinations: Compressed images to "/storage/compressed/{format}/", metadata to MongoDB collection "image_metadata"
- Side Outputs: Compression statistics to InfluxDB, failed processing to Kafka topic "image-processing-errors"
- Event Time: Use uploadTime field with 10-second watermark delay
- Parallelism: Source=2, Processing=16, Sink=8

================================================================================
(II) PARTIALLY MISSING QUERIES WITH INCOMPLETE BUT STRUCTURED PROMPTS
================================================================================

1. Basic Word Count Pipeline (Partially Missing):
Create a Flink streaming application for text processing:
- Source: Kafka topic for text messages
- Processing: Split messages into words and count frequencies
- Output: Write results to file system
- Need to implement proper windowing and state management
- Include error handling for malformed data

2. CSV Data Transformation (Partially Missing):
Implement an ETL pipeline using Flink:
- Read CSV files with employee data
- Transform records to JSON format with field mappings
- Publish to Kafka topic for downstream consumption
- Apply data validation but specific rules not defined
- Configure appropriate parallelism settings

3. Simple Log Aggregator (Partially Missing):
Build a log analysis pipeline:
- Consume log events from Kafka
- Extract log levels and aggregate by time windows
- Store results in database
- Handle late-arriving events
- Missing specific window size and watermark configuration

4. Event Filtering Pipeline (Partially Missing):
Develop an event filtering system:
- Process events from Kafka stream
- Apply filtering rules based on event attributes
- Route filtered and rejected events to different outputs
- Support runtime configuration updates
- Filter criteria and routing logic need to be specified

5. Temperature Monitoring (Partially Missing):
Create a temperature monitoring system:
- Process sensor readings from data stream
- Detect temperature anomalies and generate alerts
- Include sensor location and timing information
- Output alerts to downstream systems
- Alert thresholds and severity levels not specified

6. Industrial Equipment Predictive Maintenance (Partially Missing):
Implement a predictive maintenance pipeline:
- Process factory sensor data including temperature and vibration
- Perform trend detection and anomaly analysis
- Calculate failure risk scores using sensor inputs
- Generate maintenance alerts when risk exceeds threshold
- Missing specific algorithms for trend detection and risk calculation

7. Real-Time Chat Filter (Partially Missing):
Build a chat moderation pipeline:
- Process chat messages from streaming source
- Apply content filtering for inappropriate content
- Route messages based on filter results
- Track filtering effectiveness metrics
- Content filtering rules and severity classification undefined

8. Real-Time Image Compression (Partially Missing):
Create an image processing pipeline:
- Process image streams with metadata
- Apply compression algorithms to optimize file sizes
- Extract image characteristics and quality metrics
- Route images based on compression results
- Specific compression techniques and routing criteria not defined
